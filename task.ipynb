{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a6824ad",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d862cc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./.venv/lib/python3.10/site-packages (4.52.0.dev0)\n",
      "Requirement already satisfied: datasets in ./.venv/lib/python3.10/site-packages (3.4.1)\n",
      "Requirement already satisfied: sentencepiece in ./.venv/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: evaluate in ./.venv/lib/python3.10/site-packages (0.4.3)\n",
      "Requirement already satisfied: accelerate in ./.venv/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./.venv/lib/python3.10/site-packages (from transformers) (0.30.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.10/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in ./.venv/lib/python3.10/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.10/site-packages (from datasets) (3.11.14)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.10/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./.venv/lib/python3.10/site-packages (from accelerate) (2.5.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets sentencepiece evaluate accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aafd374a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in ./.venv/lib/python3.10/site-packages (0.4.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in ./.venv/lib/python3.10/site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.10/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in ./.venv/lib/python3.10/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./.venv/lib/python3.10/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./.venv/lib/python3.10/site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.10/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in ./.venv/lib/python3.10/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in ./.venv/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in ./.venv/lib/python3.10/site-packages (from evaluate) (0.30.1)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (19.0.1)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.11.14)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas->evaluate) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efc28c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge-score in ./.venv/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in ./.venv/lib/python3.10/site-packages (from rouge-score) (2.2.2)\n",
      "Requirement already satisfied: nltk in ./.venv/lib/python3.10/site-packages (from rouge-score) (3.9.1)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (from rouge-score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in ./.venv/lib/python3.10/site-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.10/site-packages (from nltk->rouge-score) (8.1.8)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.10/site-packages (from nltk->rouge-score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.10/site-packages (from nltk->rouge-score) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.10/site-packages (from nltk->rouge-score) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b37a46fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in ./.venv/lib/python3.10/site-packages (0.15.0)\n",
      "Requirement already satisfied: accelerate in ./.venv/lib/python3.10/site-packages (1.5.2)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: sentencepiece in ./.venv/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.10/site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from peft) (24.2)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.10/site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.10/site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in ./.venv/lib/python3.10/site-packages (from peft) (2.5.1)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.10/site-packages (from peft) (4.52.0.dev0)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.10/site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: safetensors in ./.venv/lib/python3.10/site-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in ./.venv/lib/python3.10/site-packages (from peft) (0.30.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.10/site-packages (from huggingface_hub>=0.25.0->peft) (2024.12.0)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.10/site-packages (from huggingface_hub>=0.25.0->peft) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in ./.venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.10/site-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.10/site-packages (from transformers->peft) (0.21.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.1.31)\n",
      "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.45.5\n"
     ]
    }
   ],
   "source": [
    "!pip install peft accelerate bitsandbytes sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34d302d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in ./.venv/lib/python3.10/site-packages (0.45.5)\n",
      "Requirement already satisfied: torch<3,>=2.0 in ./.venv/lib/python3.10/site-packages (from bitsandbytes) (2.5.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.venv/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.venv/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.venv/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.venv/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.venv/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.venv/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.venv/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.venv/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.venv/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.venv/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.venv/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in ./.venv/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59eeed11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torch>=1.0.0 in ./.venv/lib/python3.10/site-packages (from bert-score) (2.5.1)\n",
      "Requirement already satisfied: pandas>=1.0.1 in ./.venv/lib/python3.10/site-packages (from bert-score) (2.2.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in ./.venv/lib/python3.10/site-packages (from bert-score) (4.52.0.dev0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (from bert-score) (1.26.4)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from bert-score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in ./.venv/lib/python3.10/site-packages (from bert-score) (4.67.1)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.10/site-packages (from bert-score) (3.10.1)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.10/site-packages (from bert-score) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.venv/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.venv/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.venv/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.venv/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.venv/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.venv/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.venv/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.venv/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.venv/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.venv/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.venv/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in ./.venv/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./.venv/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (0.30.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (0.5.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.10/site-packages (from matplotlib->bert-score) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.10/site-packages (from matplotlib->bert-score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.10/site-packages (from matplotlib->bert-score) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.10/site-packages (from matplotlib->bert-score) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.10/site-packages (from matplotlib->bert-score) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.10/site-packages (from matplotlib->bert-score) (3.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->bert-score) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests->bert-score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->bert-score) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->bert-score) (2025.1.31)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert-score) (2.1.5)\n",
      "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "Installing collected packages: bert-score\n",
      "Successfully installed bert-score-0.3.13\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de880fc8",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8733e38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce2f3942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/SSD2/mahmoudreda/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e083be",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd75e1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load only the required languages\n",
    "xl_sum_en = pd.DataFrame(load_dataset(\"csebuetnlp/xlsum\", \"english\", split=\"train\"))[:10000]\n",
    "xl_sum_es = pd.DataFrame(load_dataset(\"csebuetnlp/xlsum\", \"spanish\", split=\"train\"))[:10000]\n",
    "xl_sum_zh =  pd.DataFrame(load_dataset(\"csebuetnlp/xlsum\", \"chinese_simplified\", split=\"train\"))[:10000]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc28210b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([xl_sum_en, xl_sum_es,xl_sum_zh], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8cae8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = train_test_split(dataset, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9414b681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27000, 3000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "len(train),len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6287af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Dataset.from_pandas(train)\n",
    "test = Dataset.from_pandas(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0147291",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04a97ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 27000/27000 [00:11<00:00, 2408.21 examples/s]\n",
      "Map: 100%|██████████| 3000/3000 [00:01<00:00, 2587.52 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_checkpoint = \"facebook/bart-large-cnn\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "max_input_length = 512\n",
    "max_target_length = 128\n",
    "\n",
    "\n",
    "def preprocess(example):\n",
    "    inputs = example[\"text\"]\n",
    "    targets = example[\"summary\"]\n",
    "\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=max_target_length, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "train = train.map(preprocess, batched=True, remove_columns=[\"text\", \"summary\", \"title\", \"url\", \"id\"])\n",
    "test = test.map(preprocess, batched=True, remove_columns=[\"text\", \"summary\", \"title\", \"url\", \"id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06822610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 27000\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c824ed31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 3000\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b93b4f",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92a145bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbnb_config = BitsAndBytesConfig(\\n    load_in_4bit=True,\\n    bnb_4bit_quant_type=\"nf4\",     # best for QLoRA\\n    bnb_4bit_use_double_quant=True,\\n    bnb_4bit_compute_dtype=torch.float16,\\n)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",     # best for QLoRA\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258ba9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961a31c3",
   "metadata": {},
   "source": [
    "# Rouge Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba82f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # Use the same tokenizer used during training\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    # Compute BERTScore\n",
    "    bertscore_result = bertscore.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "           lang=\"multilingual\"\n",
    "    )\n",
    "\n",
    "    result = {key: round(value * 100, 4) for key, value in result.items()}\n",
    "    result[\"gen_len\"] = np.mean([len(tokenizer.tokenize(pred)) for pred in decoded_preds])\n",
    "    result[\"bertscore_f1\"] = round(np.mean(bertscore_result[\"f1\"]) * 100, 4)\n",
    "    result[\"bertscore_precision\"] = round(np.mean(bertscore_result[\"precision\"]) * 100, 4)\n",
    "    result[\"bertscore_recall\"] = round(np.mean(bertscore_result[\"recall\"]) * 100, 4)\n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "395af1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable P2P and IB communication\n",
    "import os\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f437e120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lora_config = LoraConfig(\\n    r=8,\\n    lora_alpha=16,\\n    target_modules=[\"q\", \"v\"],  # or [\"k\", \"q\", \"v\"] depending on attention blocks\\n    lora_dropout=0.05,\\n    bias=\"none\",\\n    task_type=TaskType.SEQ_2_SEQ_LM\\n)\\n\\nmodel = get_peft_model(model, lora_config)\\nmodel.print_trainable_parameters()  # Should show only a few LoRA layers trainable'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q\", \"v\"],  # or [\"k\", \"q\", \"v\"] depending on attention blocks\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()  # Should show only a few LoRA layers trainable\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169f477a",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f441cf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e13783",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1817157/1919534951.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./bart-multilingual-summarizer-new-new\",\n",
    "    eval_strategy=\"steps\",\n",
    "    learning_rate=5e-5,\n",
    "    eval_steps=1000,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps = 1000,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,            \n",
    "    metric_for_best_model=\"rougeL\",         \n",
    "greater_is_better=True,   \n",
    "    generation_max_length=128,   # ✅ sets generation limits\n",
    "    generation_num_beams=4       # ✅ sets beam search for better outputs\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=test,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator = collator\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf5c5a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10125' max='10125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10125/10125 1:44:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "      <th>Bertscore F1</th>\n",
       "      <th>Bertscore Precision</th>\n",
       "      <th>Bertscore Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.273900</td>\n",
       "      <td>1.276647</td>\n",
       "      <td>20.526000</td>\n",
       "      <td>6.054300</td>\n",
       "      <td>14.396000</td>\n",
       "      <td>14.384200</td>\n",
       "      <td>79.094333</td>\n",
       "      <td>69.886000</td>\n",
       "      <td>69.205300</td>\n",
       "      <td>70.702100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.240600</td>\n",
       "      <td>1.217741</td>\n",
       "      <td>20.707400</td>\n",
       "      <td>6.192200</td>\n",
       "      <td>14.577000</td>\n",
       "      <td>14.549000</td>\n",
       "      <td>63.158667</td>\n",
       "      <td>69.682700</td>\n",
       "      <td>69.701000</td>\n",
       "      <td>69.817900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.213300</td>\n",
       "      <td>1.179448</td>\n",
       "      <td>20.527300</td>\n",
       "      <td>5.916300</td>\n",
       "      <td>14.131800</td>\n",
       "      <td>14.120500</td>\n",
       "      <td>69.398000</td>\n",
       "      <td>69.468600</td>\n",
       "      <td>69.220500</td>\n",
       "      <td>69.845300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.014900</td>\n",
       "      <td>1.157119</td>\n",
       "      <td>20.948700</td>\n",
       "      <td>6.357300</td>\n",
       "      <td>14.750900</td>\n",
       "      <td>14.715400</td>\n",
       "      <td>70.332333</td>\n",
       "      <td>70.107100</td>\n",
       "      <td>69.739100</td>\n",
       "      <td>70.624600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.963100</td>\n",
       "      <td>1.143168</td>\n",
       "      <td>21.277000</td>\n",
       "      <td>6.435500</td>\n",
       "      <td>14.992000</td>\n",
       "      <td>14.960100</td>\n",
       "      <td>75.328000</td>\n",
       "      <td>70.232400</td>\n",
       "      <td>69.581200</td>\n",
       "      <td>71.032400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.981900</td>\n",
       "      <td>1.134357</td>\n",
       "      <td>21.602800</td>\n",
       "      <td>6.763900</td>\n",
       "      <td>15.187400</td>\n",
       "      <td>15.184500</td>\n",
       "      <td>68.886333</td>\n",
       "      <td>70.151000</td>\n",
       "      <td>69.787100</td>\n",
       "      <td>70.679100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.859200</td>\n",
       "      <td>1.136339</td>\n",
       "      <td>21.420600</td>\n",
       "      <td>6.572100</td>\n",
       "      <td>15.018000</td>\n",
       "      <td>15.000300</td>\n",
       "      <td>70.765333</td>\n",
       "      <td>70.407400</td>\n",
       "      <td>69.886500</td>\n",
       "      <td>71.098800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.823300</td>\n",
       "      <td>1.136148</td>\n",
       "      <td>21.465400</td>\n",
       "      <td>6.573200</td>\n",
       "      <td>15.093800</td>\n",
       "      <td>15.095900</td>\n",
       "      <td>67.596000</td>\n",
       "      <td>70.073200</td>\n",
       "      <td>69.746700</td>\n",
       "      <td>70.562600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.868100</td>\n",
       "      <td>1.122890</td>\n",
       "      <td>21.875600</td>\n",
       "      <td>6.864700</td>\n",
       "      <td>15.453900</td>\n",
       "      <td>15.425200</td>\n",
       "      <td>71.496667</td>\n",
       "      <td>70.394100</td>\n",
       "      <td>69.857700</td>\n",
       "      <td>71.099300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.858700</td>\n",
       "      <td>1.122647</td>\n",
       "      <td>21.928300</td>\n",
       "      <td>7.008200</td>\n",
       "      <td>15.515900</td>\n",
       "      <td>15.502900</td>\n",
       "      <td>72.574000</td>\n",
       "      <td>70.478600</td>\n",
       "      <td>69.893300</td>\n",
       "      <td>71.228700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/SSD2/mahmoudreda/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10125, training_loss=1.0448691087887612, metrics={'train_runtime': 6248.3017, 'train_samples_per_second': 12.964, 'train_steps_per_second': 1.62, 'total_flos': 8.7767736385536e+16, 'train_loss': 1.0448691087887612, 'epoch': 3.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainer.train()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0e7b41",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72549e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./continue-finetuned-model/tokenizer_config.json',\n",
       " './continue-finetuned-model/special_tokens_map.json',\n",
       " './continue-finetuned-model/vocab.json',\n",
       " './continue-finetuned-model/merges.txt',\n",
       " './continue-finetuned-model/added_tokens.json',\n",
       " './continue-finetuned-model/tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./continue-finetuned-model\")\n",
    "trainer.tokenizer.save_pretrained(\"./continue-finetuned-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a9920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./bart-multilingual-final\")\n",
    "tokenizer.save_pretrained(\"./bart-multilingual-final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0ac30e",
   "metadata": {},
   "source": [
    "# Push to Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac175ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/SSD2/mahmoudreda/.venv/lib/python3.10/site-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_path = \"/home/SSD2/mahmoudreda/bart-multilingual-final\"\n",
    "\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273cfe24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/illm/lib/python3.12/site-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_path = \"/home/SSD2/mahmoudreda/bart-multilingual-final\"\n",
    "\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bf90002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/illm/lib/python3.12/site-packages/transformers/modeling_utils.py:2758: UserWarning: Moving the following attributes in the config to the generation config: {'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "978502d365f84b28946026b910a4dc4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2157dfb74e724bb6ace066b81eb68734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Mahmoud3899/bart-multilingual-final/commit/4e88fea7f30959f023c2e47df280d6eb2f7cd409', commit_message='Upload tokenizer', commit_description='', oid='4e88fea7f30959f023c2e47df280d6eb2f7cd409', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Mahmoud3899/bart-multilingual-final', endpoint='https://huggingface.co', repo_type='model', repo_id='Mahmoud3899/bart-multilingual-final'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"Mahmoud3899/bart-multilingual-final\")\n",
    "tokenizer.push_to_hub(\"Mahmoud3899/bart-multilingual-final\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba66858",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7087533e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/SSD2/mahmoudreda/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/SSD2/mahmoudreda/.venv/lib/python3.10/site-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"bart-multilingual-final\", tokenizer=\"bart-multilingual-final\",device=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87e631bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 306522/306522 [00:01<00:00, 187657.48 examples/s]\n",
      "Generating test split: 100%|██████████| 11535/11535 [00:00<00:00, 213038.68 examples/s]\n",
      "Generating validation split: 100%|██████████| 11535/11535 [00:00<00:00, 205068.06 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11535"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "test =  pd.DataFrame(load_dataset(\"csebuetnlp/xlsum\", \"english\", split=\"train\"))\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08c014d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'By Kate DaileyBBC News Earlier this week, Trump posted a photo of himself sitting at a desk at Mar-a-Largo, a permanent marker hovering over a notepad. \"Writing my inaugural address at the Winter White House, Mar-a-Lago, three weeks ago. Looking forward to Friday,\" he tweeted. Trump vows to end \\'American carnage\\' Trump\\'s angry call to arms Full text of Trump\\'s inauguration speech It\\'s unclear whether the president-elect actually wrote the speech himself, but the content was pure Trump: the same populist message that resonated throughout the primaries and the campaign. \"Today, we are not merely transferring power from one administration to another, or from one party to another, but we are transferring power from Washington, DC, and giving it back to you, the people,\" he said at the beginning of his remarks. For some on Twitter, it bore an eerie similarity to the Batman villain Bane\\'s speech in The Dark Night Rises, so much so that someone posted a 10-second mash-up of the two. But such snarky reactions, warned Fox News commentator Guy Benson, underestimate how popular his rhetoric is with Trump supporters. \"People panning the speech still don\\'t seem to understand how resonant the \\'I will never ignore you\\' theme has been, and still is,\" he wrote, referencing Trump\\'s many callouts to those who feel left out of American progress. Trump spoke of a country whose citizens had too long been ignored by the coastal elite: \"Their victories have not been your victories. Their triumphs have not been your triumphs. And while they celebrated in our nation\\'s capital, there was little to celebrate for struggling families all across our land.\" He painted a picture of a broken and damaged country, dotted with rusting-out factories \"like tombstones\", city streets plagued with \"crime and the gangs, and the drugs that have stolen too many lives,\" and the wealth of the middle class \"ripped from their homes and then redistributed all across the world\". It was an unusually bleak speech for an inaugural address. One democratic strategist called it \"startlingly angry\". Conservatives were more mixed. According to MSNBC host Joe Scarborough, the speech was not intended to follow tradition: \"Donald Trump\\'s speech was not an inaugural address. It was a primal scream aimed at Washington, DC.\" Author Hugh Hewitt called it \"authentic, determined, almost grim\". He wrote, \"I expected more joy, but it cannot be said that POTUS @realDonaldTrump said anything he hasn\\'t said before. He has a plan and it\\'s going to roll out fast.\" Others were sceptical of the breadth of those plans. Trump said the country was poised to \"free the earth from the miseries of disease, and to harness the energies, industries and technologies of tomorrow\", as well as \"eradicate from the face of the Earth\" radical Islamic terrorism. Writer Ben Shapiro expressed doubt about Trump\\'s plans to both take power away from DC, and use his position as President to steer trade and create jobs. \"These cannot both be true,\" he wrote. Many also noted that it\\'s easy to campaign as an outsider, railing about America\\'s problems, but harder to lead, when one must find solutions. \"After three months in which Trump is president and it\\'s still the same Washington, that speech is going to seem wildly imprudent,\" wrote Noah Rothman, assistant editor at Commentary Magazine. Commentator Mary Katherine Hahn thinks voters aren\\'t interested in sweeping rhetoric. \"I am unabashedly ideological. The country is not. His message is populist & popular. His opponents dismiss that at their political peril.\" Pollster Frank Luntz said President Trump seemed to pivot, if not in tone then at least in substance: \"President Trump\\'s inaugural speech was the best delivery I\\'ve ever seen from him.\" A more well-known conservative kept mum on his opinion. When the Washington Post asked George W Bush what he thought of the speech, he merely replied, \"Good to see you.\" One high-profile Twitter user was an unabashed fan. Former Ku Klux Klan leader David Duke tweeted multiple times in favour of Trump\\'s speech.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f1e8951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Donald Trump campaigned on becoming a president unlike any Washington has ever seen. With his inauguration speech, he's already set the tone.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"summary\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "379e8d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'Donald Trump\\'s inaugural speech was a call to arms for Americans to come together and fight for what he described as \"the cause\" of America\\'s problems. It was an unusually bleak speech for a president who campaigned as an outsider, and was met with boos and boos from the crowd that greeted his arrival at the Lincoln Memorial. But what did his supporters make of it?'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(test[\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94f0a1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38110"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "test =  pd.DataFrame(load_dataset(\"csebuetnlp/xlsum\", \"spanish\", split=\"train\"))\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11f76615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'El ejército le había dado un ultimátum al presidente Morsi: 48 horas para escuchar las demandas de la gente. No está claro que rol tomará el actual presidente Mohamed Morsi durante este proceso y en el período previo a una elección presidencial libre. La agencia de noticias Reuters ha recibido información similar de fuentes militares, que subrayan que todavía se está discutiendo el plan y que podría cambiar. Las Fuerzas Armadas de Egipto advirtieron el lunes que intervendrían si el presidente Morsi no escuchaba las demandas de la gente dentro de las 48 horas. Esto después de que millones de personas salieran a las calles para exigir la renuncia del presidente.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"text\"][9000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9ed1866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La directora de ayuda humanitaria de Naciones Unidas, Valerie Amos, advirtió que de no invertir más dinero, el Programa Mundial de Alimentos tendrá que detener sus operaciones en Siria en dos meses.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"summary\"][9000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e75678f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'os en Egipto informan que el ejército le había dado un ultimátum al presidente Mohamed Morsi: 48 horas para escuchar las demandas de la gente, según fuentes.'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(test[\"text\"][9000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fd3ebaa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37362"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "test =  pd.DataFrame(load_dataset(\"csebuetnlp/xlsum\", \"chinese_simplified\", split=\"train\"))\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7c0ff46d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'软件巨头微软公司的IE6网络浏览器在美国的用户不到美国互联网用户的1%，该公司为此举行了一个随意的庆祝仪式。 微软公司急于终止使用老一代的网络搜索器，鼓励用户升级使用IE8或9网络浏览器。 与此同时，微软公司的对手谷歌被迫减少在网络宣传他们自己的Chrome网络搜索器。'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "230756fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'微软公司做蛋糕庆祝该公司的IE6网络浏览器终止使用。'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"summary\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a85ad31c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': '网络搜索器（IE6）的微软公司，提出的鼓励用户。'}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(test[\"text\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9146bb89",
   "metadata": {},
   "source": [
    "# Human Feedback & cultural/linguistic biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654e1f14",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0c27a1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_text = \"Donald Trump delivered a speech that focused on the struggles of ordinary Americans and promised to return power to the people.\"\n",
    "spanish_text = \"Donald Trump pronunció un discurso centrado en las luchas de los estadounidenses comunes y prometió devolver el poder al pueblo.\"\n",
    "mandarin_text = \"唐纳德·特朗普发表演讲，强调普通美国人的困境，并承诺将权力还给人民。\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ab9e3305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 60, but your input_length is only 24. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=12)\n",
      "Your max_length is set to 60, but your input_length is only 42. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=21)\n"
     ]
    }
   ],
   "source": [
    "summary_en = summarizer(english_text, max_length=60, min_length=20, do_sample=False)[0]['summary_text']\n",
    "summary_es = summarizer(spanish_text, max_length=60, min_length=20, do_sample=False)[0]['summary_text']\n",
    "summary_zh = summarizer(mandarin_text, max_length=60, min_length=20, do_sample=False)[0]['summary_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8eb461",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare English vs Spanish summary\n",
    "bert_result_en_es = bertscore.compute(predictions=[summary_en], references=[summary_es], lang=\"multilingual\")\n",
    "bert_result_en_zh = bertscore.compute(predictions=[summary_en], references=[summary_zh], lang=\"multilingual\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8a1a0393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BERTSCORE SIMILARITY ===\n",
      "EN ↔ ES BERTScore F1: 69.08\n",
      "EN ↔ ZH BERTScore F1: 67.35\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== BERTSCORE SIMILARITY ===\")\n",
    "print(\"EN ↔ ES BERTScore F1:\", round(bert_result_en_es['f1'][0] * 100, 2))\n",
    "print(\"EN ↔ ZH BERTScore F1:\", round(bert_result_en_zh['f1'][0] * 100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1a5ee1",
   "metadata": {},
   "source": [
    "## Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3ea5557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "english_text = \"The Spanish government has announced a new economic plan to reduce youth unemployment. The Economy Minister stated that the goal is to create 250,000 new jobs in the next two years through tax incentives for small and medium-sized businesses. There is also a plan for significant investment in vocational training programs for youth.\"\n",
    "spanish_text = \"El gobierno español ha anunciado un nuevo plan económico para reducir el desempleo juvenil. El ministro de Economía declaró que el objetivo es crear 250.000 nuevos puestos de trabajo en los próximos dos años mediante incentivos fiscales para las pequeñas y medianas empresas. También se prevé una inversión significativa en programas de formación profesional para jóvenes.\"\n",
    "mandarin_text = \"西班牙政府宣布了一项新的经济计划，旨在降低青年失业率。经济部长表示，目标是在未来两年内通过对中小企业的税收激励措施创造25万个新的就业岗位。该计划还包括对青年职业培训项目的大规模投资。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "935c6a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_en = summarizer(english_text, max_length=60, min_length=20, do_sample=False)[0]['summary_text']\n",
    "summary_es = summarizer(spanish_text, max_length=60, min_length=20, do_sample=False)[0]['summary_text']\n",
    "summary_zh = summarizer(mandarin_text, max_length=60, min_length=20, do_sample=False)[0]['summary_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adbdc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare English vs Spanish summary\n",
    "bert_result_es_en = bertscore.compute(predictions=[summary_es], references=[summary_en], lang=\"multilingual\")\n",
    "bert_result_es_zh = bertscore.compute(predictions=[summary_es], references=[summary_zh], lang=\"multilingual\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8a8c8d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BERTSCORE SIMILARITY ===\n",
      "ES ↔ EN BERTScore F1: 67.3\n",
      "ES ↔ ZH BERTScore F1: 67.27\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== BERTSCORE SIMILARITY ===\")\n",
    "print(\"ES ↔ EN BERTScore F1:\", round(bert_result_es_en['f1'][0] * 100, 2))\n",
    "print(\"ES ↔ ZH BERTScore F1:\", round(bert_result_es_zh['f1'][0] * 100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c5ebac",
   "metadata": {},
   "source": [
    "## Chinese "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0e4f326f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "english_text = \"China's Cyberspace Administration has recently issued a new regulation requiring all major tech companies to report to regulators before launching new algorithmic services. The measure aims to strengthen oversight of AI and recommendation systems to protect user privacy and prevent information manipulation. This regulation is seen as an important step in China's efforts to enhance internet governance.\"\n",
    "spanish_text = \"La Administración del Ciberespacio de China ha emitido recientemente una nueva regulación que exige a todas las principales empresas tecnológicas informar a los reguladores antes de lanzar nuevos servicios basados en algoritmos. La medida tiene como objetivo reforzar la supervisión de la inteligencia artificial y los sistemas de recomendación para proteger la privacidad de los usuarios y prevenir la manipulación de la información. Esta regulación se considera un paso importante en los esfuerzos de China por mejorar la gobernanza de Internet.\"\n",
    "mandarin_text = \"中国国家网信办近日发布了一项新的规定，要求所有大型科技公司在推出新的算法服务前，必须向监管机构报告。该措施旨在加强对人工智能和推荐算法的监管，以保护用户隐私并防止信息操控。这一规定被认为是中国在加强互联网治理方面迈出的重要一步。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9f091765",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_en = summarizer(english_text, max_length=60, min_length=20, do_sample=False)[0]['summary_text']\n",
    "summary_es = summarizer(spanish_text, max_length=60, min_length=20, do_sample=False)[0]['summary_text']\n",
    "summary_zh = summarizer(mandarin_text, max_length=60, min_length=20, do_sample=False)[0]['summary_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "35f2a58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_result_zh_es = bertscore.compute(predictions=[summary_zh], references=[summary_en], lang=\"multilingual\")\n",
    "bert_result_eh_en = bertscore.compute(predictions=[summary_zh], references=[summary_es], lang=\"multilingual\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fb17a589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BERTSCORE SIMILARITY ===\n",
      "ES ↔ EN BERTScore F1: 66.24\n",
      "ES ↔ ZH BERTScore F1: 61.66\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== BERTSCORE SIMILARITY ===\")\n",
    "print(\"ES ↔ EN BERTScore F1:\", round(bert_result_zh_es['f1'][0] * 100, 2))\n",
    "print(\"ES ↔ ZH BERTScore F1:\", round(bert_result_eh_en['f1'][0] * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2f7800",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "illm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
